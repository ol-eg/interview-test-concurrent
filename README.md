# interview-test-concurrent

### Тестовое задание:

Написать программу на Python, которая делает следующие действия:

1. Создает 50 zip-архивов, в каждом 100 xml файлов со случайными данными следующей структуры:
```xml
<root>
  <var name=’id’ value=’<случайное уникальное строковое значение>’/>
  <var name=’level’ value=’<случайное число от 1 до 100>’/>
  <objects>
    <object name=’<случайное строковое значение>’/>
    <object name=’<случайное строковое значение>’/>
    …
  </objects>
</root>
```
В тэге objects случайное число (от 1 до 10) вложенных тэгов object.
2. Обрабатывает директорию с полученными zip архивами, разбирает вложенные xml файлы и формирует 2 csv файла:

..Первый: id, level - по одной строке на каждый xml файл
..Второй: id, object_name - по отдельной строке для каждого тэга object (получится от 1 до 10 строк на каждый xml файл)

Очень желательно сделать так, чтобы задание 2 эффективно использовало ресурсы многоядерного процессора.
Также желательно чтобы программа работала быстро.

### Комментарии

1. Задание выполнено используя Linux OS, Python 3.7. Должно работать и на Windows с Python 3.x но не тестировал.
2. Код покрыт unit тестами слабо, так как это не упомянуто специально в задании
3. Самая медленная часть программы - создание архивов (на моем лаптопе 0.5сек один архив).
Решение выбранно использовать multiprocessing с уровнем параллизма равному числу cpu cores, и использовать все ресурсы
машины на создание архивов, затем читать архивы так же в паралельном режиме. В случае больших xml файлов
возможно другая стратегия была бы боллее эффективна - asyncio, и consumer/provider шаблон.

```bash
$ git clone https://github.com/ol-eg/interview-test-concurrent.git
$ cd interview-test-concurrent
$ python3 main.py
```

Архивы пишуться в ```./data/archives```, csv файлы в ```./data/csv```
